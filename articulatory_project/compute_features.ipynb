{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.io.wavfile as wf\n",
    "from python_speech_features import mfcc\n",
    "from python_speech_features import delta\n",
    "from python_speech_features import logfbank\n",
    "from scipy import signal\n",
    "import scipy.io.wavfile as wav\n",
    "import neurokit as nk\n",
    "import os\n",
    "from numpy import zeros, floor, log10, log, mean, array, sqrt, vstack, cumsum,ones, log2, std\n",
    "import glob\n",
    "import scipy.io\n",
    "import librosa\n",
    "import numpy as np\n",
    "import re\n",
    "import pywt\n",
    "import sklearn \n",
    "import gc\n",
    "#from voice_detection import VoiceActivityDetector as VAD\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import keras\n",
    "from keras.layers import Input,Dense\n",
    "from keras.models import Model\n",
    "#from tsfresh import extract_features\n",
    "from numpy.fft import fft\n",
    "from numpy import zeros, floor, log10, log, mean, array, sqrt, vstack, cumsum, ones, log2, std\n",
    "from numpy.linalg import svd, lstsq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_mat(file_path,channels):\n",
    "    \"\"\"\n",
    "    Purpose : loading and preprocess .mat EEG files\n",
    "    Arg:\n",
    "        file_path : file directory\n",
    "        channels : number of eeg channels processed in .mat file\n",
    "    Output:\n",
    "        df : dataframe for the EEG signals\n",
    "    \"\"\"\n",
    "    mat = scipy.io.loadmat(file_path)\n",
    "    mat = {k:v for k, v in mat.items() if k[0]!='_'}\n",
    "    names = list(mat.keys())\n",
    "    df = pd.DataFrame()\n",
    "    for key in names[:channels]:\n",
    "        value = mat[key]\n",
    "        value = pd.DataFrame(value)\n",
    "        df = pd.merge(df,value,how='right',right_index=True,left_index=True)\n",
    "    df.columns = names[:channels]\n",
    "    return df\n",
    "def get_mfcc(file_path,sr_):\n",
    "    \"\"\"\n",
    "    Purpose : compute mfcc features from a audio file\n",
    "    Arg:\n",
    "        file_path : audio file\n",
    "    Output:\n",
    "        mf : dataframe for mfcc features\n",
    "    \"\"\"\n",
    "#     (rate,sig) = wav.read(file_path)\n",
    "#     mfcc_feat = mfcc(sig,rate,winstep=0.01)\n",
    "    df = pd.DataFrame()\n",
    "    for file in range(30):\n",
    "        path = file_path + str(file) + \".wav\"\n",
    "        y, sr = librosa.load(path,sr = sr_)\n",
    "        mfcc_feat = librosa.feature.mfcc(y=y, sr=sr_,hop_length = 1, n_mfcc=13)\n",
    "        mf = pd.DataFrame(mfcc_feat)\n",
    "        mf = np.transpose(mf)\n",
    "        df = pd.concat([df,mf])\n",
    "    return df\n",
    "def get_articulatory(file_path):\n",
    "    \"\"\"\n",
    "    Purpose: load and preprocess the articulatory .csv for 30 sentences\n",
    "    Arg:\n",
    "        file_path: folder of 30 sentences for one subject\n",
    "    Output: \n",
    "        articulatory df\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    for file in range(30):\n",
    "        path = file_path + str(file) + \".csv\"\n",
    "        sent_df = pd.read_csv(path)\n",
    "        sent_df.columns = [\"a1\", \"a2\", \"a3\", \"a4\", \"a5\", \"a6\"]\n",
    "        sent_df.loc[:,\"Sentence\"] = file\n",
    "        df = pd.concat([df, sent_df])\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def get_audio_signal(file_path,sr):\n",
    "    \"\"\"\n",
    "    Purpose : compute audio signal from a audio file\n",
    "    Arg:\n",
    "        file_path : audio file\n",
    "    Output:\n",
    "        y : dataframe for audio signal features\n",
    "    \"\"\"\n",
    "    y, sr = librosa.load(file_path,sr=sr)\n",
    "    y = pd.DataFrame(y)\n",
    "    return y\n",
    "def trimm_mfcc(file_path,label):\n",
    "    \"\"\"\n",
    "    Trimming the mfcc by the label mapping\n",
    "    Arg:\n",
    "        file_path : the mfcc path\n",
    "        label : label_mapping.csv\n",
    "    Output:\n",
    "        mf_df : trimmed mfcc file\n",
    "    \"\"\"\n",
    "    mf_df = get_mfcc(file_path)\n",
    "    pre_len = 0\n",
    "    mf_df['code'] = None\n",
    "    mf_df['word'] = None\n",
    "    for row in range(len(label)):\n",
    "        dif,code,word = label.iloc[row][['dif','code','word']]\n",
    "        dif = round(dif,1)\n",
    "        if dif==0:\n",
    "            dif = 0.1\n",
    "        if pre_len ==0:\n",
    "            start = int(pre_len)*100\n",
    "            end = int(pre_len*100)+int(dif*100)-1\n",
    "        else:\n",
    "            start = pre_len\n",
    "            end = pre_len + int(dif*100)\n",
    "        mf_df.at[start:end,['code','word']] = code,word\n",
    "        pre_len = end\n",
    "    mf_df = mf_df[mf_df['code'].notnull()]\n",
    "    return mf_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "y,sr = librosa.core.load(\"audio/jay1/1.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)//sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10312, 13)\n",
      "(10222, 7)\n",
      "             a1        a2        a3        a4        a5        a6  Sentence\n",
      "0      0.086838 -0.121088 -1.244090 -1.067202 -0.316126 -0.763496         0\n",
      "1      0.177641 -0.142149 -1.293557 -1.054068 -0.368612 -0.721482         0\n",
      "2      0.267745 -0.168539 -1.340196 -1.035811 -0.415383 -0.674794         0\n",
      "3      0.343848 -0.194861 -1.371688 -1.022014 -0.441479 -0.627153         0\n",
      "4      0.397213 -0.214886 -1.381066 -1.021195 -0.440705 -0.581629         0\n",
      "...         ...       ...       ...       ...       ...       ...       ...\n",
      "10217 -0.467356 -0.712224  0.102272  0.412311  0.333031 -0.191934        29\n",
      "10218 -0.525288 -0.657675  0.132488  0.338855  0.420778 -0.172223        29\n",
      "10219 -0.564131 -0.598585  0.184833  0.289414  0.482768 -0.124834        29\n",
      "10220 -0.592099 -0.534039  0.249298  0.256945  0.531249 -0.062952        29\n",
      "10221 -0.621053 -0.463667  0.316935  0.232585  0.580827 -0.000242        29\n",
      "\n",
      "[10222 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# EEG_data = loading_mat(\"EEG_data/jay1_Filters.mat\", 31)\n",
    "# label = pd.read_csv(\"marker_data/dataframes/jay1_df.csv\")\n",
    "mfcc = get_mfcc(\"audio/jay1/\", 100)\n",
    "artic = get_articulatory(\"Articulatory_data/jay1/\")\n",
    "print(mfcc.shape)\n",
    "print(artic.shape)\n",
    "print(artic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gautam-admin/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:376: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/home/gautam-admin/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:576: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fp1</th>\n",
       "      <th>Fz</th>\n",
       "      <th>F3</th>\n",
       "      <th>F7</th>\n",
       "      <th>FT9</th>\n",
       "      <th>FC5</th>\n",
       "      <th>FC1</th>\n",
       "      <th>C3</th>\n",
       "      <th>T7</th>\n",
       "      <th>TP9</th>\n",
       "      <th>...</th>\n",
       "      <th>CP2</th>\n",
       "      <th>C4</th>\n",
       "      <th>T8</th>\n",
       "      <th>FT10</th>\n",
       "      <th>FC6</th>\n",
       "      <th>FC2</th>\n",
       "      <th>F4</th>\n",
       "      <th>F8</th>\n",
       "      <th>Fp2</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>30.273312</td>\n",
       "      <td>29.359789</td>\n",
       "      <td>35.896530</td>\n",
       "      <td>46.829239</td>\n",
       "      <td>47.339523</td>\n",
       "      <td>32.360237</td>\n",
       "      <td>28.087755</td>\n",
       "      <td>37.642586</td>\n",
       "      <td>24.123953</td>\n",
       "      <td>33.641674</td>\n",
       "      <td>...</td>\n",
       "      <td>5.103919</td>\n",
       "      <td>30.150871</td>\n",
       "      <td>33.061317</td>\n",
       "      <td>44.900940</td>\n",
       "      <td>30.919868</td>\n",
       "      <td>26.091274</td>\n",
       "      <td>23.977362</td>\n",
       "      <td>23.272577</td>\n",
       "      <td>13.109600</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>30.380035</td>\n",
       "      <td>29.846155</td>\n",
       "      <td>36.348316</td>\n",
       "      <td>46.720852</td>\n",
       "      <td>48.055157</td>\n",
       "      <td>33.151192</td>\n",
       "      <td>28.164606</td>\n",
       "      <td>37.502579</td>\n",
       "      <td>24.842512</td>\n",
       "      <td>37.457886</td>\n",
       "      <td>...</td>\n",
       "      <td>5.422882</td>\n",
       "      <td>30.509935</td>\n",
       "      <td>32.228752</td>\n",
       "      <td>44.955849</td>\n",
       "      <td>30.558807</td>\n",
       "      <td>26.383636</td>\n",
       "      <td>24.464115</td>\n",
       "      <td>22.853870</td>\n",
       "      <td>14.160234</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>29.958183</td>\n",
       "      <td>30.112015</td>\n",
       "      <td>36.628563</td>\n",
       "      <td>46.263279</td>\n",
       "      <td>48.433105</td>\n",
       "      <td>33.455353</td>\n",
       "      <td>28.154905</td>\n",
       "      <td>37.264664</td>\n",
       "      <td>25.448910</td>\n",
       "      <td>38.727966</td>\n",
       "      <td>...</td>\n",
       "      <td>5.580903</td>\n",
       "      <td>30.940941</td>\n",
       "      <td>31.286001</td>\n",
       "      <td>45.114067</td>\n",
       "      <td>30.062483</td>\n",
       "      <td>26.632214</td>\n",
       "      <td>24.785841</td>\n",
       "      <td>22.421146</td>\n",
       "      <td>14.666398</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>28.958513</td>\n",
       "      <td>30.049828</td>\n",
       "      <td>36.601723</td>\n",
       "      <td>45.405296</td>\n",
       "      <td>48.501381</td>\n",
       "      <td>33.260632</td>\n",
       "      <td>28.015406</td>\n",
       "      <td>36.958488</td>\n",
       "      <td>25.871763</td>\n",
       "      <td>37.241631</td>\n",
       "      <td>...</td>\n",
       "      <td>5.557716</td>\n",
       "      <td>31.375088</td>\n",
       "      <td>30.286585</td>\n",
       "      <td>45.248978</td>\n",
       "      <td>29.405685</td>\n",
       "      <td>26.715206</td>\n",
       "      <td>24.805420</td>\n",
       "      <td>21.981754</td>\n",
       "      <td>14.375419</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>27.493750</td>\n",
       "      <td>29.616899</td>\n",
       "      <td>36.217869</td>\n",
       "      <td>44.195816</td>\n",
       "      <td>48.346493</td>\n",
       "      <td>32.680389</td>\n",
       "      <td>27.724592</td>\n",
       "      <td>36.632881</td>\n",
       "      <td>26.112572</td>\n",
       "      <td>33.760189</td>\n",
       "      <td>...</td>\n",
       "      <td>5.386588</td>\n",
       "      <td>31.724718</td>\n",
       "      <td>29.252106</td>\n",
       "      <td>45.208443</td>\n",
       "      <td>28.569763</td>\n",
       "      <td>26.563412</td>\n",
       "      <td>24.424519</td>\n",
       "      <td>21.505655</td>\n",
       "      <td>13.236904</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102753</td>\n",
       "      <td>-18.416998</td>\n",
       "      <td>12.270480</td>\n",
       "      <td>5.748578</td>\n",
       "      <td>-17.894714</td>\n",
       "      <td>1.981005</td>\n",
       "      <td>4.008342</td>\n",
       "      <td>11.487144</td>\n",
       "      <td>7.120096</td>\n",
       "      <td>3.779440</td>\n",
       "      <td>14.997946</td>\n",
       "      <td>...</td>\n",
       "      <td>10.846295</td>\n",
       "      <td>10.992861</td>\n",
       "      <td>19.594849</td>\n",
       "      <td>28.552872</td>\n",
       "      <td>12.464072</td>\n",
       "      <td>14.388573</td>\n",
       "      <td>3.185489</td>\n",
       "      <td>16.343039</td>\n",
       "      <td>7.583976</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102754</td>\n",
       "      <td>-18.784754</td>\n",
       "      <td>11.971295</td>\n",
       "      <td>5.628751</td>\n",
       "      <td>-17.874226</td>\n",
       "      <td>1.316536</td>\n",
       "      <td>4.321395</td>\n",
       "      <td>11.582186</td>\n",
       "      <td>7.912390</td>\n",
       "      <td>3.899096</td>\n",
       "      <td>17.698071</td>\n",
       "      <td>...</td>\n",
       "      <td>10.860739</td>\n",
       "      <td>10.252621</td>\n",
       "      <td>17.136614</td>\n",
       "      <td>26.796537</td>\n",
       "      <td>11.108159</td>\n",
       "      <td>14.446994</td>\n",
       "      <td>2.209593</td>\n",
       "      <td>15.283985</td>\n",
       "      <td>7.404121</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102755</td>\n",
       "      <td>-19.456444</td>\n",
       "      <td>11.546941</td>\n",
       "      <td>5.198599</td>\n",
       "      <td>-18.143768</td>\n",
       "      <td>0.194912</td>\n",
       "      <td>4.337282</td>\n",
       "      <td>11.643970</td>\n",
       "      <td>8.430668</td>\n",
       "      <td>3.633068</td>\n",
       "      <td>21.086306</td>\n",
       "      <td>...</td>\n",
       "      <td>10.753442</td>\n",
       "      <td>9.457969</td>\n",
       "      <td>14.500813</td>\n",
       "      <td>24.532343</td>\n",
       "      <td>9.511013</td>\n",
       "      <td>14.429818</td>\n",
       "      <td>1.154181</td>\n",
       "      <td>13.838909</td>\n",
       "      <td>6.852047</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102756</td>\n",
       "      <td>-20.326025</td>\n",
       "      <td>11.024794</td>\n",
       "      <td>4.550863</td>\n",
       "      <td>-18.678337</td>\n",
       "      <td>-1.174308</td>\n",
       "      <td>4.084603</td>\n",
       "      <td>11.602949</td>\n",
       "      <td>8.500950</td>\n",
       "      <td>3.064466</td>\n",
       "      <td>24.233812</td>\n",
       "      <td>...</td>\n",
       "      <td>10.473352</td>\n",
       "      <td>8.794523</td>\n",
       "      <td>12.387587</td>\n",
       "      <td>22.322336</td>\n",
       "      <td>7.977871</td>\n",
       "      <td>14.339578</td>\n",
       "      <td>0.243596</td>\n",
       "      <td>12.269120</td>\n",
       "      <td>6.037446</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102757</td>\n",
       "      <td>-21.251707</td>\n",
       "      <td>10.471900</td>\n",
       "      <td>3.840587</td>\n",
       "      <td>-19.376434</td>\n",
       "      <td>-2.572715</td>\n",
       "      <td>3.625843</td>\n",
       "      <td>11.427780</td>\n",
       "      <td>8.035587</td>\n",
       "      <td>2.323800</td>\n",
       "      <td>26.213034</td>\n",
       "      <td>...</td>\n",
       "      <td>10.042503</td>\n",
       "      <td>8.408314</td>\n",
       "      <td>11.217021</td>\n",
       "      <td>20.556870</td>\n",
       "      <td>6.770006</td>\n",
       "      <td>14.196322</td>\n",
       "      <td>-0.342076</td>\n",
       "      <td>10.810229</td>\n",
       "      <td>5.101028</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102758 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Fp1         Fz         F3         F7        FT9        FC5  \\\n",
       "0       30.273312  29.359789  35.896530  46.829239  47.339523  32.360237   \n",
       "1       30.380035  29.846155  36.348316  46.720852  48.055157  33.151192   \n",
       "2       29.958183  30.112015  36.628563  46.263279  48.433105  33.455353   \n",
       "3       28.958513  30.049828  36.601723  45.405296  48.501381  33.260632   \n",
       "4       27.493750  29.616899  36.217869  44.195816  48.346493  32.680389   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "102753 -18.416998  12.270480   5.748578 -17.894714   1.981005   4.008342   \n",
       "102754 -18.784754  11.971295   5.628751 -17.874226   1.316536   4.321395   \n",
       "102755 -19.456444  11.546941   5.198599 -18.143768   0.194912   4.337282   \n",
       "102756 -20.326025  11.024794   4.550863 -18.678337  -1.174308   4.084603   \n",
       "102757 -21.251707  10.471900   3.840587 -19.376434  -2.572715   3.625843   \n",
       "\n",
       "              FC1         C3         T7        TP9  ...        CP2         C4  \\\n",
       "0       28.087755  37.642586  24.123953  33.641674  ...   5.103919  30.150871   \n",
       "1       28.164606  37.502579  24.842512  37.457886  ...   5.422882  30.509935   \n",
       "2       28.154905  37.264664  25.448910  38.727966  ...   5.580903  30.940941   \n",
       "3       28.015406  36.958488  25.871763  37.241631  ...   5.557716  31.375088   \n",
       "4       27.724592  36.632881  26.112572  33.760189  ...   5.386588  31.724718   \n",
       "...           ...        ...        ...        ...  ...        ...        ...   \n",
       "102753  11.487144   7.120096   3.779440  14.997946  ...  10.846295  10.992861   \n",
       "102754  11.582186   7.912390   3.899096  17.698071  ...  10.860739  10.252621   \n",
       "102755  11.643970   8.430668   3.633068  21.086306  ...  10.753442   9.457969   \n",
       "102756  11.602949   8.500950   3.064466  24.233812  ...  10.473352   8.794523   \n",
       "102757  11.427780   8.035587   2.323800  26.213034  ...  10.042503   8.408314   \n",
       "\n",
       "               T8       FT10        FC6        FC2         F4         F8  \\\n",
       "0       33.061317  44.900940  30.919868  26.091274  23.977362  23.272577   \n",
       "1       32.228752  44.955849  30.558807  26.383636  24.464115  22.853870   \n",
       "2       31.286001  45.114067  30.062483  26.632214  24.785841  22.421146   \n",
       "3       30.286585  45.248978  29.405685  26.715206  24.805420  21.981754   \n",
       "4       29.252106  45.208443  28.569763  26.563412  24.424519  21.505655   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "102753  19.594849  28.552872  12.464072  14.388573   3.185489  16.343039   \n",
       "102754  17.136614  26.796537  11.108159  14.446994   2.209593  15.283985   \n",
       "102755  14.500813  24.532343   9.511013  14.429818   1.154181  13.838909   \n",
       "102756  12.387587  22.322336   7.977871  14.339578   0.243596  12.269120   \n",
       "102757  11.217021  20.556870   6.770006  14.196322  -0.342076  10.810229   \n",
       "\n",
       "              Fp2  Sentence  \n",
       "0       13.109600         0  \n",
       "1       14.160234         0  \n",
       "2       14.666398         0  \n",
       "3       14.375419         0  \n",
       "4       13.236904         0  \n",
       "...           ...       ...  \n",
       "102753   7.583976        29  \n",
       "102754   7.404121        29  \n",
       "102755   6.852047        29  \n",
       "102756   6.037446        29  \n",
       "102757   5.101028        29  \n",
       "\n",
       "[102758 rows x 32 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def trim_EEG(EEG_data, marker_data):\n",
    "    output = []\n",
    "    df = pd.DataFrame()\n",
    "    for row in range(len(marker_data)):\n",
    "        start, end = marker_data.iloc[row]\n",
    "        EEG_start = int(start * 1000)\n",
    "        EEG_end = int(end * 1000)\n",
    "        window = EEG_data.iloc[EEG_start+1:EEG_end-1,]\n",
    "        window.loc[:,\"Sentence\"] = row\n",
    "        #output.append(window)\n",
    "        df = pd.concat([df, window])\n",
    "    return df.reset_index(drop=True)\n",
    "trim = trim_EEG(EEG_data, label)\n",
    "trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 102758 entries, 0 to 102757\n",
      "Data columns (total 31 columns):\n",
      "Fp1     102758 non-null float64\n",
      "Fz      102758 non-null float64\n",
      "F3      102758 non-null float64\n",
      "F7      102758 non-null float64\n",
      "FT9     102758 non-null float64\n",
      "FC5     102758 non-null float64\n",
      "FC1     102758 non-null float64\n",
      "C3      102758 non-null float64\n",
      "T7      102758 non-null float64\n",
      "TP9     102758 non-null float64\n",
      "CP5     102758 non-null float64\n",
      "CP1     102758 non-null float64\n",
      "Pz      102758 non-null float64\n",
      "P3      102758 non-null float64\n",
      "P7      102758 non-null float64\n",
      "O1      102758 non-null float64\n",
      "Oz      102758 non-null float64\n",
      "O2      102758 non-null float64\n",
      "P4      102758 non-null float64\n",
      "P8      102758 non-null float64\n",
      "TP10    102758 non-null float64\n",
      "CP6     102758 non-null float64\n",
      "CP2     102758 non-null float64\n",
      "C4      102758 non-null float64\n",
      "T8      102758 non-null float64\n",
      "FT10    102758 non-null float64\n",
      "FC6     102758 non-null float64\n",
      "FC2     102758 non-null float64\n",
      "F4      102758 non-null float64\n",
      "F8      102758 non-null float64\n",
      "Fp2     102758 non-null float64\n",
      "dtypes: float64(31)\n",
      "memory usage: 24.3 MB\n",
      "None\n",
      "column Fp1 is completed\n",
      "column Fz is completed\n",
      "column F3 is completed\n",
      "column F7 is completed\n",
      "column FT9 is completed\n",
      "column FC5 is completed\n",
      "column FC1 is completed\n",
      "column C3 is completed\n",
      "column T7 is completed\n",
      "column TP9 is completed\n",
      "column CP5 is completed\n",
      "column CP1 is completed\n",
      "column Pz is completed\n",
      "column P3 is completed\n",
      "column P7 is completed\n",
      "column O1 is completed\n",
      "column Oz is completed\n",
      "column O2 is completed\n",
      "column P4 is completed\n",
      "column P8 is completed\n",
      "column TP10 is completed\n",
      "column CP6 is completed\n",
      "column CP2 is completed\n",
      "column C4 is completed\n",
      "column T8 is completed\n",
      "column FT10 is completed\n",
      "column FC6 is completed\n",
      "column FC2 is completed\n",
      "column F4 is completed\n",
      "column F8 is completed\n",
      "column Fp2 is completed\n"
     ]
    }
   ],
   "source": [
    "features = feature_compute(trim.iloc[:,:-1], 10, trim.iloc[:,-1])\n",
    "jay1_feats = features.features_computing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10276, 155)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jay1_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class feature_compute():\n",
    "    def __init__(self, data, window_size,label):\n",
    "        self.data = data\n",
    "        self.window_size = window_size\n",
    "        self.label = label\n",
    "        #self.map_file = map_file\n",
    "\n",
    "    def spectral_entropy(self, col_data):\n",
    "        \"\"\"\n",
    "        Purpose : compute spectral entropy accross the data\n",
    "        Arg:\n",
    "            data : input EEG signals data\n",
    "            window_sze : moving window size\n",
    "        Output:\n",
    "            df : results after computing moving spectral entropy\n",
    "        \"\"\"\n",
    "        data = col_data\n",
    "        window_size = self.window_size\n",
    "        df = []\n",
    "        for i in range(int(len(data)/window_size)):\n",
    "            complexity = nk.complexity(data[i*window_size:(i+1)*window_size],spectral=True,shannon=False,sampen=False,multiscale=False,svd = False,correlation=False,\n",
    "                              higushi = False,petrosian=False,fisher = False,hurst = False,dfa= False,lyap_r=False,lyap_e =False)\n",
    "            df.append(complexity['Entropy_Spectral'])\n",
    "        df = pd.DataFrame(df)\n",
    "        return df.reset_index(drop=True)\n",
    "    def kurtosis(self, col_data):\n",
    "        \"\"\"\n",
    "        Purpose : compute kurtosis accross the data\n",
    "        Arg:\n",
    "            data : input EEG signals data\n",
    "            window_sze : moving window size\n",
    "        Output:\n",
    "            df : results after computing moving spectral entropy\n",
    "        \"\"\"    \n",
    "        data = col_data\n",
    "        window_size = self.window_size\n",
    "        df=[]\n",
    "        for i in range(int(len(data)/window_size)):\n",
    "            df.append(data[i*window_size:(i+1)*window_size].kurtosis())\n",
    "        df = pd.DataFrame(df)\n",
    "        return df.reset_index(drop=True)\n",
    "    def average(self, col_data):\n",
    "        \"\"\"\n",
    "        Purpose : compute moving average accross the data\n",
    "        Arg:\n",
    "            data : input EEG signals data\n",
    "            window_sze : moving window size\n",
    "        Output:\n",
    "            df : results after computing moving spectral entropy\n",
    "        \"\"\"\n",
    "        data = col_data\n",
    "        window_size = self.window_size\n",
    "        df = pd.DataFrame()\n",
    "        for i in range(int(len(data)/window_size)):\n",
    "            dt = np.array(data[i*window_size:(i+1)*window_size])\n",
    "            window = np.ones((window_size,))/float(window_size)\n",
    "            dt = pd.DataFrame(np.convolve(dt, window,mode='valid'))\n",
    "            df = pd.concat([df,dt])\n",
    "        return df.reset_index(drop=True)\n",
    "\n",
    "    def root_mean_square(self, col_data):\n",
    "        \"\"\"\n",
    "        Purpose : compute root mean square accross the data\n",
    "        Arg:\n",
    "            data : input EEG signals data\n",
    "            window_sze : moving window size\n",
    "        Output:\n",
    "            df : results after computing moving spectral entropy\n",
    "        \"\"\"\n",
    "        data = col_data\n",
    "        window_size = self.window_size\n",
    "        df = pd.DataFrame()\n",
    "        for i in range(int(len(data)/window_size)):\n",
    "            dt = np.power(data[i*window_size:(i+1)*window_size],2)\n",
    "            window = np.ones((window_size,))/float(window_size)\n",
    "            dt = pd.DataFrame(np.sqrt(np.convolve(dt, window,mode='valid')))\n",
    "            df = pd.concat([df,dt])\n",
    "        return df.reset_index(drop=True)\n",
    "    def zero_crossing_rate(self, col_data):\n",
    "        \"\"\"\n",
    "        Purpose : compute root mean square accross the data\n",
    "        Arg:\n",
    "            data : input EEG signals data\n",
    "            window_sze : moving window size\n",
    "        Output:\n",
    "            df : results after computing moving spectral entropy\n",
    "        \"\"\"\n",
    "        data = col_data.values\n",
    "        window_size = self.window_size\n",
    "        df = pd.DataFrame()\n",
    "        df = librosa.feature.zero_crossing_rate(data,frame_length=100,hop_length=10)\n",
    "        df = pd.DataFrame(np.transpose(df))\n",
    "        return df.reset_index(drop=True)\n",
    "    def features_computing(self, trimmed = False, type_tf = 0):\n",
    "        \"\"\"\n",
    "        Purpose : take the EEG signals data and computing moving features\n",
    "        Arg:\n",
    "            self:\n",
    "        Output:\n",
    "            df: the dataframe with 5 statistical features for each channels\n",
    "        \"\"\"\n",
    "        if trimmed == True:\n",
    "            data = self.trimmed_data(rate = 1000)\n",
    "            labeling = data[['code','word']]\n",
    "            data = data.drop(columns=['code','word'])\n",
    "        else:\n",
    "            data = self.data\n",
    "        window_size = self.window_size\n",
    "        df = pd.DataFrame()\n",
    "        df_step = pd.DataFrame()\n",
    "        print(data.info())\n",
    "        if type_tf == 0:\n",
    "            for column in data.columns:\n",
    "                col_data = data[column]\n",
    "                zero = self.zero_crossing_rate(col_data)\n",
    "                rms = self.root_mean_square(col_data)\n",
    "                avg = self.average(col_data)\n",
    "                kurt = self.kurtosis(col_data)\n",
    "                entropy = self.spectral_entropy(col_data)\n",
    "                df_step = pd.concat([zero,rms,avg,kurt,entropy],axis=1)\n",
    "                df_step.columns = [column+'_'+ feature for feature in ['zero','rms','avg','kurt','entropy']]\n",
    "                df = pd.concat([df,df_step], axis=1)\n",
    "                print('column %s is completed'%(column))\n",
    "        df = df.reset_index(drop=True)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(df,encoding_dim):\n",
    "    \"\"\"\n",
    "        Encoding \n",
    "    \"\"\"\n",
    "    X = df.values\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    ncol = X.shape[1]\n",
    "\n",
    "    input_dim = Input(shape = (ncol, ))\n",
    "    # DEFINE THE DIMENSION OF ENCODER ASSUMED 3\n",
    "    encoding_dim = encoding_dim\n",
    "    # DEFINE THE ENCODER LAYERS\n",
    "\n",
    "    encoded1 = Dense(155, activation = 'relu')(input_dim)\n",
    "    encoded2 = Dense(100, activation = 'relu')(encoded1)\n",
    "    encoded3 = Dense(55, activation = 'relu')(encoded2)\n",
    "    encoded4 = Dense(30, activation = 'relu')(encoded3)\n",
    "    encoded5 = Dense(15, activation = 'relu')(encoded4)\n",
    "    encoded6 = Dense(encoding_dim, activation = 'relu')(encoded5)\n",
    "    # DEFINE THE DECODER LAYERS\n",
    "\n",
    "    decoded1 = Dense(15, activation = 'relu')(encoded5)\n",
    "    decoded2 = Dense(30, activation = 'relu')(decoded1)\n",
    "    decoded3 = Dense(55, activation = 'relu')(decoded2)\n",
    "    decoded4 = Dense(100, activation = 'relu')(decoded3)\n",
    "    decoded5 = Dense(155, activation = 'sigmoid')(decoded4)\n",
    "    decoded6 = Dense(ncol, activation = 'relu')(encoded5)\n",
    "    # COMBINE ENCODER AND DECODER INTO AN AUTOENCODER MODEL\n",
    "    autoencoder = Model(input = input_dim, output = decoded5)\n",
    "    # CONFIGURE AND TRAIN THE AUTOENCODER\n",
    "    autoencoder.compile(optimizer = 'adam', loss = 'binary_crossentropy')\n",
    "    autoencoder.summary()\n",
    "    autoencoder.fit(X, X, nb_epoch = 500, batch_size = 100, shuffle = True, validation_data = (X, X))\n",
    "    # THE ENCODER TO EXTRACT THE REDUCED DIMENSION FROM THE ABOVE AUTOENCODER\n",
    "    encoder = Model(input = input_dim, output = encoded5)\n",
    "    encoded_input = Input(shape = (encoding_dim, ))\n",
    "    encoded_out = encoder.predict(X)\n",
    "    encoded_out = pd.DataFrame(encoded_out)\n",
    "#     encoded_out = pd.concat([encoded_out,y],axis=1)\n",
    "    return encoded_out\n",
    "def pad_sequences(sequences, maxlen=None, dtype=np.float32,\n",
    "                  padding='post', truncating='post', value=0.):\n",
    "    '''Pads each sequence to the same length: the length of the longest\n",
    "    sequence.\n",
    "        If maxlen is provided, any sequence longer than maxlen is truncated to\n",
    "        maxlen. Truncation happens off either the beginning or the end\n",
    "        (default) of the sequence. Supports post-padding (default) and\n",
    "        pre-padding.\n",
    "        Args:\n",
    "            sequences: list of lists where each element is a sequence\n",
    "            maxlen: int, maximum length\n",
    "            dtype: type to cast the resulting sequence.\n",
    "            padding: 'pre' or 'post', pad either before or after each sequence.\n",
    "            truncating: 'pre' or 'post', remove values from sequences larger\n",
    "            than maxlen either in the beginning or in the end of the sequence\n",
    "            value: float, value to pad the sequences to the desired value.\n",
    "        Returns\n",
    "            x: numpy array with dimensions (number_of_sequences, maxlen)\n",
    "            lengths: numpy array with the original sequence lengths\n",
    "    '''\n",
    "    lengths = np.asarray([len(s) for s in sequences], dtype=np.int64)\n",
    "\n",
    "    nb_samples = len(sequences)\n",
    "    if maxlen is None:\n",
    "        maxlen = np.max(lengths)\n",
    "\n",
    "    # take the sample shape from the first non empty sequence\n",
    "    # checking for consistency in the main loop below.\n",
    "    sample_shape = tuple()\n",
    "    for s in sequences:\n",
    "        if len(s) > 0:\n",
    "            sample_shape = np.asarray(s).shape[1:]\n",
    "            break\n",
    "\n",
    "    x = (np.ones((nb_samples, maxlen) + sample_shape) * value).astype(dtype)\n",
    "    for idx, s in enumerate(sequences):\n",
    "        if len(s) == 0:\n",
    "            continue  # empty list was found\n",
    "        if truncating == 'pre':\n",
    "            trunc = s[-maxlen:]\n",
    "        elif truncating == 'post':\n",
    "            trunc = s[:maxlen]\n",
    "        else:\n",
    "            raise ValueError('Truncating type \"%s\" not understood' % truncating)\n",
    "\n",
    "        # check `trunc` has expected shape\n",
    "        trunc = np.asarray(trunc, dtype=dtype)\n",
    "        if trunc.shape[1:] != sample_shape:\n",
    "            raise ValueError('Shape of sample %s of sequence at position %s is different from expected shape %s' %\n",
    "                             (trunc.shape[1:], idx, sample_shape))\n",
    "\n",
    "        if padding == 'post':\n",
    "            x[idx, :len(trunc)] = trunc\n",
    "        elif padding == 'pre':\n",
    "            x[idx, -len(trunc):] = trunc\n",
    "        else:\n",
    "            raise ValueError('Padding type \"%s\" not understood' % padding)\n",
    "    return x, lengths"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
